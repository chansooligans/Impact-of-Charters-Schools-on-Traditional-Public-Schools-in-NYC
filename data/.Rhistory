# Generate Potential Outcomes
y_0 = 0.4*z1 + 0.5*z2 + 0.3*z3 + 0.9*z4 + 0.1*z5 + z6 + rnorm(n,0,1)
y_1 = 0.4*z1 + 0.5*z2 + 0.3*z3 + 0.9*z4 + 0.1*z5 + z6 + eff + rnorm(n,0,1)
y = y_0*(1-treat) + y_1*treat
# Generate Researcher Dataset
df[[4]] = as.data.frame(cbind(y,y_1,y_0, treat, Z_4))
df[[4]] = df[[4]] %>% arrange(-treat)
# SATE
SATE[[4]] = mean(y_1) - mean(y_0)
SATE[[4]]
# Pick Simulation
mod_match = eb.out = m.out = cbps.over = cbps.just = list()
for(k in 1:4){
formula = as.formula(paste('treat ~ ',paste(confounders[[k]],collapse=' + '),sep=''))
#### (1)
# Logistic Regression, 1-1 Matching
mod_match[[k]] = weightit(formula, data = df[[k]], method = "ps", estimand = "ATT")
df[[k]]$wt = mod_match[[k]]$weights
#### (2)
# CBPS Weights (OVER)
cbps.over[[k]]= weightit(formula, data = df[[k]], method = "cbps", estimand = "ATT", over=TRUE)
df[[k]]$wt.cbps = cbps.over[[k]]$weights
#### (3)
# CBPS Weights (JUST)
cbps.just[[k]]= weightit(formula, data = df[[k]], method = "cbps", estimand = "ATT", over=FALSE)
df[[k]]$wt.cbps = cbps.over[[k]]$weights
#### (4)
# Entropy Balancing Weights
eb.out[[k]] = ebalance(Treatment = df[[k]]$treat, X = df[[k]][,confounders[[k]]])
df[[k]]$wt.eb = 1
df[[k]][df[[k]]$treat == 0,'wt.eb'] = eb.out[[k]]$w
}
baltab.log = baltab.cb.over = baltab.cb.just = baltab.eb = love.plots = list()
for(k in 1:4){
baltab.log[[k]] = bal.tab(data = df[[k]],
covs = df[[k]][,confounders[[k]]],
treat = df[[k]]$treat,
weights = df[[k]]$wt,
method = 'weighting',
disp.v.ratio = TRUE,
un = TRUE,
m.threshold = 0.1,
v.threshold = 1.1,
estimand = 'ATT')
baltab.cb.over[[k]] = bal.tab(cbps.over[[k]],
disp.v.ratio = TRUE,
un = TRUE,
m.threshold = 0.1,
v.threshold = 1.1,
estimand = 'ATT')
baltab.cb.just[[k]] = bal.tab(cbps.just[[k]],
disp.v.ratio = TRUE,
un = TRUE,
m.threshold = 0.1,
v.threshold = 1.1,
estimand = 'ATT')
baltab.eb[[k]] = bal.tab(eb.out[[k]],
treat = df[[k]]$treat,
covs = df[[k]][,confounders[[k]]],
disp.v.ratio = TRUE,
un = TRUE,
m.threshold = 0.1,
v.threshold = 1.1,
estimand = 'ATT')
# Initialize List of Plots
love.plots[[k]] = list()
# Plots
love.plots[[k]][[1]] = love.plot(baltab.log[[k]], threshold = 0.1) +
ggtitle('covariate balance: baseline') +
theme(plot.title = element_text(size=8),
axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank()) +
xlim(0,1)
love.plots[[k]][[2]] = love.plot(baltab.cb.over[[k]], threshold = 0.1) +
ggtitle('covariate balance: cbps over') +
theme(plot.title = element_text(size=8),
axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank()) +
xlim(0,1)
love.plots[[k]][[3]] = love.plot(baltab.cb.just[[k]], threshold = 0.1) +
ggtitle('covariate balance: cbps just') +
theme(plot.title = element_text(size=8),
axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank()) +
xlim(0,1)
love.plots[[k]][[4]] = love.plot(baltab.eb[[k]], threshold = 0.1) +
ggtitle('covariate balance: eb') +
theme(plot.title = element_text(size=8),
axis.title.x = element_text(size=6)) +
xlim(0,1)
}
grid.arrange(grobs=love.plots[[1]], ncol=1, nrow = 4,
top="Simulation 1")
grid.arrange(grobs=love.plots[[2]], ncol=1, nrow = 4,
top="Simulation 2")
grid.arrange(grobs=love.plots[[3]], ncol=1, nrow = 4,
top="Simulation 3")
grid.arrange(grobs=love.plots[[4]], ncol=1, nrow = 4,
top="Simulation 4")
balplots = list()
for(k in 1:4){
balplots[[k]] = list()
# Unadjusted plots
balplots[[k]][[1]] = bal.plot(mod_match[[k]],treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z1",which='unadjusted',type='histogram') +
guides(fill=FALSE) +
theme(plot.title = element_text(size=8)) +
ggtitle('z1 Unadjusted')
balplots[[k]][[5]] = bal.plot(mod_match[[k]],treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z2",which='unadjusted',type='histogram') +
guides(fill=FALSE) +
theme(plot.title = element_text(size=8)) +
ggtitle('z2 Unadjusted')
balplots[[k]][[9]] = bal.plot(mod_match[[k]],treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z3",which='unadjusted',type='histogram') +
guides(fill=FALSE) +
theme(plot.title = element_text(size=8)) +
ggtitle('z3 Unadjusted')
balplots[[k]][[13]] = bal.plot(mod_match[[k]],treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z4",which='unadjusted',type='histogram') +
guides(fill=FALSE) +
theme(plot.title = element_text(size=8)) +
ggtitle('z4 Unadjusted')
if(k>=3){
balplots[[k]][[17]] = bal.plot(mod_match[[k]],treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z5",which='unadjusted',type='histogram') +
guides(fill=FALSE) +
theme(plot.title = element_text(size=8)) +
ggtitle('z5 Unadjusted')
balplots[[k]][[21]] = bal.plot(mod_match[[k]],treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z6",which='unadjusted',type='histogram') +
guides(fill=FALSE) +
theme(plot.title = element_text(size=8)) +
ggtitle('z6 Unadjusted')
}
# Adjusted (Baseline)
balplots[[k]][[2]] = bal.plot(mod_match[[k]],treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z1",type='histogram') +
guides(fill=FALSE) +
theme(plot.title = element_text(size=8)) +
ggtitle('Baseline')
balplots[[k]][[6]] = bal.plot(mod_match[[k]],treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z2",type='histogram') +
guides(fill=FALSE) +
theme(plot.title = element_text(size=8)) +
ggtitle('Baseline')
balplots[[k]][[10]] = bal.plot(mod_match[[k]],treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z3",type='histogram') +
guides(fill=FALSE) +
theme(plot.title = element_text(size=8)) +
ggtitle('Baseline')
balplots[[k]][[14]] = bal.plot(mod_match[[k]],treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z4",type='histogram') +
guides(fill=FALSE) +
theme(plot.title = element_text(size=8)) +
ggtitle('Baseline')
if(k>=3){
balplots[[k]][[18]] = bal.plot(mod_match[[k]],treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z5",type='histogram') +
guides(fill=FALSE) +
theme(plot.title = element_text(size=8)) +
ggtitle('Baseline')
balplots[[k]][[22]] = bal.plot(mod_match[[k]],treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z6",type='histogram') +
guides(fill=FALSE) +
theme(plot.title = element_text(size=8)) +
ggtitle('Baseline')
}
# Adjusted (CBPS)
balplots[[k]][[3]] = bal.plot(cbps.over[[k]],treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z1",type='histogram') +
guides(fill=FALSE) +
theme(plot.title = element_text(size=8)) +
ggtitle('CBPS')
balplots[[k]][[7]] = bal.plot(cbps.over[[k]],treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z2",type='histogram') +
guides(fill=FALSE) +
theme(plot.title = element_text(size=8)) +
ggtitle('CBPS')
balplots[[k]][[11]] = bal.plot(cbps.over[[k]],treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z3",type='histogram') +
guides(fill=FALSE) +
theme(plot.title = element_text(size=8)) +
ggtitle('CBPS')
balplots[[k]][[15]] = bal.plot(cbps.over[[k]],treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z4",type='histogram') +
guides(fill=FALSE) +
theme(plot.title = element_text(size=8)) +
ggtitle('CBPS')
if(k>=3){
balplots[[k]][[19]] = bal.plot(cbps.over[[k]],treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z5",type='histogram') +
guides(fill=FALSE) +
theme(plot.title = element_text(size=8)) +
ggtitle('CBPS')
balplots[[k]][[23]] = bal.plot(cbps.over[[k]],treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z6",type='histogram') +
guides(fill=FALSE) +
theme(plot.title = element_text(size=8)) +
ggtitle('CBPS')
}
# Adjusted (Entropy Balancing)
balplots[[k]][[4]] = bal.plot(eb.out[[k]],treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z1",type='histogram') +
guides(fill=FALSE) +
theme(plot.title = element_text(size=8)) +
ggtitle('EB')
balplots[[k]][[8]] = bal.plot(eb.out[[k]],treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z2",type='histogram') +
guides(fill=FALSE) +
theme(plot.title = element_text(size=8)) +
ggtitle('EB')
balplots[[k]][[12]] = bal.plot(eb.out[[k]],treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z3",type='histogram') +
guides(fill=FALSE) +
theme(plot.title = element_text(size=8)) +
ggtitle('EB')
balplots[[k]][[16]] = bal.plot(eb.out[[k]],treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z4",type='histogram') +
guides(fill=FALSE) +
theme(plot.title = element_text(size=8)) +
ggtitle('EB')
if(k>=3){
balplots[[k]][[20]] = bal.plot(eb.out[[k]],treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z5",type='histogram') +
guides(fill=FALSE) +
theme(plot.title = element_text(size=8)) +
ggtitle('EB')
balplots[[k]][[24]] = bal.plot(eb.out[[k]],treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z6",type='histogram') +
guides(fill=FALSE) +
theme(plot.title = element_text(size=8)) +
ggtitle('EB')
}
}
grid.arrange(grobs=balplots[[1]][1:8], ncol=4, nrow = 2,
top="Simulation 1")
grid.arrange(grobs=balplots[[1]][9:12], ncol=4, nrow = 2)
grid.arrange(grobs=balplots[[2]][1:8], ncol=4, nrow = 2,
top="Simulation 2")
grid.arrange(grobs=balplots[[2]][9:12], ncol=4, nrow = 2)
grid.arrange(grobs=balplots[[3]][1:8], ncol=4, nrow = 2,
top="Simulation 3")
grid.arrange(grobs=balplots[[3]][9:16], ncol=4, nrow = 2)
grid.arrange(grobs=balplots[[3]][17:24], ncol=4, nrow = 2)
grid.arrange(grobs=balplots[[4]][1:8], ncol=4, nrow = 2,
top="Simulation 4")
grid.arrange(grobs=balplots[[4]][9:16], ncol=4, nrow = 2)
grid.arrange(grobs=balplots[[4]][17:24], ncol=4, nrow = 2)
# k=2
#
# n_sims = 100
#
# baseline_coefs = rep(1,n_sims)
# cbps_coefs = rep(1,n_sims)
# eb_coefs = rep(1,n_sims)
# naive_est = rep(1,n_sims)
#
# for(i in 1:n_sims){
#
#   # Generate treatment vector
#   treat = as.integer(runif(n) <= e_Z)
#   y = y_0*(1-treat) + y_1*treat
#   df[[k]] = as.data.frame(cbind(y, y_1, y_0, treat, Z_[,confounders[[k]]]))
#
#   ## NAIVE-estimate
#       naive_est[i] = mean(df[[k]]$y[df[[k]]$treat==1]) - mean(df[[k]]$y[df[[k]]$treat==0])
#
#   ## Baseline: Propensity Score using Logistic Regression:
#
#       # Logistic Regression, 1-1 Matching
#       mod_match = matchit(treat ~ z1 + z2 + z3 + z4, method='nearest', data=df[[k]], replace = TRUE)
#       df[[k]]$wt = mod_match$weights
#
#   ## Covariate Balancing Propensity Score (CBPS)
#
#       # CBPS Weights
#       cbps.fit = CBPS(treat ~ ., data = df[[k]][,confounders[[k]]], ATT = TRUE)
#       m.out = matchit(treat ~ fitted(cbps.fit), method = 'nearest', data = df[[k]], replace = TRUE)
#       df[[k]]$wt.cbps = m.out$weights
#
#   ## Entropy Balancing
#
#       # Entropy Balancing Weights
#       eb.out = ebalance(Treatment = df[[k]]$treat, X = df[[k]][,confounders[[k]]])
#       df[[k]]$wt.eb = 1
#       df[[k]][df[[k]]$treat == 0,'wt.eb'] = eb.out$w
#
#   # Linear Regression
#   # mod.baseline.sim = lm(y ~ ., data = df[[k]][,c('y','treat',confounders[[k]])], weights = df[[k]]$wt)
#   # mod.cbps = lm(y ~ ., data = df[[k]][,c('y','treat',confounders[[k]])], weights = df[[k]]$wt.cbps)
#   # mod.eb = lm(y ~ ., data = df[[k]][,c('y','treat',confounders[[k]])], weights = df[[k]]$wt.eb)
#   # Save Coefficients
#   # baseline_coefs[i] = summary(mod.baseline.sim)$coefficients['treat',1]
#   # cbps_coefs[i] = summary(mod.cbps)$coefficients['treat',1]
#   # eb_coefs[i] = summary(mod.eb)$coefficients['treat',1]
#
#   baseline_coefs[i] = weighted.mean(df[[k]]$y[df[[k]]$treat==1],df[[k]]$wt[df[[k]]$treat==1]) -
#     weighted.mean(df[[k]]$y[df[[k]]$treat==0],df[[k]]$wt[df[[k]]$treat==0])
#   cbps_coefs[i] = weighted.mean(df[[k]]$y[df[[k]]$treat==1],df[[k]]$wt.cbps[df[[k]]$treat==1]) -
#     weighted.mean(df[[k]]$y[df[[k]]$treat==0],df[[k]]$wt.cbps[df[[k]]$treat==0])
#   eb_coefs[i] = weighted.mean(df[[k]]$y[df[[k]]$treat==1],df[[k]]$wt.eb[df[[k]]$treat==1]) -
#     weighted.mean(df[[k]]$y[df[[k]]$treat==0],df[[k]]$wt.eb[df[[k]]$treat==0])
# }
?save()
paste('sim',k,'.RDATA',sep='')
n_sims = 100
# Samle size
n = 500
n_sims = 100
# Inverse Logit Function
inv.logit = function (x) {
y = 1/(1+exp(-x))
return(y)
}
mod.inv.logit = function (x) {
y = 0.90/(1+exp(-x)) + 0.05
return(y)
}
df = list()
SATE = list()
confounders = list()
confounders[[1]] = paste('z',seq(1,4),sep='')
confounders[[2]] = paste('z',seq(1,4),sep='')
confounders[[3]] = paste('z',seq(1,6),sep='')
confounders[[4]] = paste('z',seq(1,6),sep='')
rm(list=ls())
# Samle size
n = 500
n_sims = 100
# Inverse Logit Function
inv.logit = function (x) {
y = 1/(1+exp(-x))
return(y)
}
mod.inv.logit = function (x) {
y = 0.90/(1+exp(-x)) + 0.05
return(y)
}
df = list()
SATE = list()
confounders = list()
confounders[[1]] = paste('z',seq(1,4),sep='')
confounders[[2]] = paste('z',seq(1,4),sep='')
confounders[[3]] = paste('z',seq(1,6),sep='')
confounders[[4]] = paste('z',seq(1,6),sep='')
# Randomly generate covariates
z1 = rnorm(n,  0, 1)
z2 = rnorm(n,  0, 1)
z3 = rnorm(n,  0, 1)
z4 = rnorm(n,  0, 1)
Z_1 = cbind(z1, z2, z3, z4)
# Estimate "True Propensity Scores"
e_Z = inv.logit(z1 + z2 + z3 + z4)
# Generate treatment vector
treat = rbinom(n,1,prob=e_Z)
table(treat)/length(treat)
# True Propensity Score Density
plot(density(e_Z[treat==0]))
lines(density(e_Z[treat==1]),col=2)
# Treatment Effect
eff = 4
# Generate Potential Outcomes
y_0 = 0.4*z1 + 0.5*z2 + 0.3*z3 + 0.9*z4 + rnorm(n,0,1)
y_1 = 0.4*z1 + 0.5*z2 + 0.3*z3 + 0.9*z4 + eff + rnorm(n,0,1)
y = y_0*(1-treat) + y_1*treat
# Generate Researcher Dataset
df[[1]] = as.data.frame(cbind(y,y_1,y_0, treat, Z_1))
df[[1]] = df[[1]] %>% arrange(-treat)
# SATE
SATE[[1]]= mean(y_1) - mean(y_0)
SATE[[1]]
# Randomly generate covariates
z1 = rnorm(n,  0, 1)
z2 = rnorm(n,  0, 1)
z3 = rnorm(n,  0, 1)
z4 = rnorm(n,  0, 1)
Z_2 = cbind(z1, z2, z3, z4)
# Estimate "True Propensity Scores"
x1 = -0.25*exp(1/z1)
x2 = z2
x3 = z3
x4 = z3*z4
x5 = z4
e_Z = inv.logit(x1 + x2 + x3 + x4 + x5)
# Generate treatment vector
treat = rbinom(n,1,prob=e_Z)
table(treat)/length(treat)
# True Propensity Score Density
plot(density(e_Z[treat==0]))
lines(density(e_Z[treat==1]),col=2)
# Treatment Effect
eff = 4
# Generate Potential Outcomes
y_0 = 0.4*z1 + 0.5*z2 + 0.3*z3 + 0.9*z4 + rnorm(n,0,1)
y_1 = 0.4*z1 + 0.5*z2 + 0.3*z3 + 0.9*z4 + eff + rnorm(n,0,1)
y = y_0*(1-treat) + y_1*treat
# Generate Researcher Dataset
df[[2]] = as.data.frame(cbind(y,y_1,y_0, treat, Z_2))
df[[2]] = df[[2]] %>% arrange(-treat)
# SATE
SATE[[2]]= mean(y_1) - mean(y_0)
SATE[[2]]
# Randomly generate covariates:
z1 = rnorm(n, -2, 1)
z2 = rnorm(n, -1, 1)
z3 = rnorm(n, -1, 1)
z4 = rpois(n, 5)
z5 = -rbinom(n, 5, 0.75)
z6 = rchisq(n, df=1)
Z_3 = cbind(z1, z2, z3, z4, z5, z6)
# Estimate "True Propensity Scores"
e_Z = inv.logit(z1 + z2 + z3 + z4 + z5 + z6)
# Generate treatment vector
treat = rbinom(n,1,prob=e_Z)
table(treat)/length(treat)
# True Propensity Score Density
plot(density(e_Z[treat==0]))
lines(density(e_Z[treat==1]),col=2)
# Treatment Effect
eff = 4
# Generate Potential Outcomes
y_0 = 0.4*z1 + 0.5*z2 + 0.3*z3 + 0.9*z4 + 0.1*z5 + z6 + rnorm(n,0,1)
y_1 = 0.4*z1 + 0.5*z2 + 0.3*z3 + 0.9*z4 + 0.1*z5 + z6 + eff + rnorm(n,0,1)
y = y_0*(1-treat) + y_1*treat
# Generate Researcher Dataset
df[[3]] = as.data.frame(cbind(y,y_1,y_0, treat, Z_3))
df[[3]] = df[[3]] %>% arrange(-treat)
# SATE
SATE[[3]] = mean(y_1) - mean(y_0)
SATE[[3]]
# Randomly generate covariates:
z1 = rnorm(n, 0, 1)
z2 = rnorm(n, 0, 1)
z3 = rbinom(n, 5, 0.75)
z4 = rpois(n, 5)
z5 = rnorm(n, -2, 2)
z6 = rchisq(n, df=1)
Z_4 = cbind(z1, z2, z3, z4, z5, z6)
# Estimate "True Propensity Scores"
x1 = exp(z1/2)
x2 = z2/(1+exp(z1))
x3 = -2*sqrt(z3)
x4 = sqrt(z3*z4)
x5 = -0.25*(z1+z5)^2
x6 = -2*z6
e_Z = inv.logit(x1 + x2 + x3 + x4 + x5 + x6)
# Generate treatment vector
treat = rbinom(n,1,prob=e_Z)
table(treat)/length(treat)
# True Propensity Score Density
plot(density(e_Z[treat==0]))
lines(density(e_Z[treat==1]),col=2)
# Treatment Effect
eff = 4
# Generate Potential Outcomes
y_0 = 0.4*z1 + 0.5*z2 + 0.3*z3 + 0.9*z4 + 0.1*z5 + z6 + rnorm(n,0,1)
y_1 = 0.4*z1 + 0.5*z2 + 0.3*z3 + 0.9*z4 + 0.1*z5 + z6 + eff + rnorm(n,0,1)
y = y_0*(1-treat) + y_1*treat
# Generate Researcher Dataset
df[[4]] = as.data.frame(cbind(y,y_1,y_0, treat, Z_4))
df[[4]] = df[[4]] %>% arrange(-treat)
# SATE
SATE[[4]] = mean(y_1) - mean(y_0)
SATE[[4]]
for(k in 1:4){
formula = as.formula(paste('treat ~ ',paste(confounders[[k]],collapse=' + '),sep=''))
#### (1)
# Logistic Regression, 1-1 Matching
mod_match[[k]] = weightit(formula, data = df[[k]], method = "ps", estimand = "ATT")
df[[k]]$wt = mod_match[[k]]$weights
#### (2)
# CBPS Weights (OVER)
cbps.over[[k]]= weightit(formula, data = df[[k]], method = "cbps", estimand = "ATT", over=TRUE)
df[[k]]$wt.cbps = cbps.over[[k]]$weights
#### (3)
# CBPS Weights (JUST)
cbps.just[[k]]= weightit(formula, data = df[[k]], method = "cbps", estimand = "ATT", over=FALSE)
df[[k]]$wt.cbps = cbps.over[[k]]$weights
#### (4)
# Entropy Balancing Weights
eb.out[[k]] = ebalance(Treatment = df[[k]]$treat, X = df[[k]][,confounders[[k]]])
df[[k]]$wt.eb = 1
df[[k]][df[[k]]$treat == 0,'wt.eb'] = eb.out[[k]]$w
}
mod_match = eb.out = m.out = cbps.over = cbps.just = list()
library(WeightIt)
library(CBPS)
library(ebal)
for(k in 1:4){
formula = as.formula(paste('treat ~ ',paste(confounders[[k]],collapse=' + '),sep=''))
#### (1)
# Logistic Regression, 1-1 Matching
mod_match[[k]] = weightit(formula, data = df[[k]], method = "ps", estimand = "ATT")
df[[k]]$wt = mod_match[[k]]$weights
#### (2)
# CBPS Weights (OVER)
cbps.over[[k]]= weightit(formula, data = df[[k]], method = "cbps", estimand = "ATT", over=TRUE)
df[[k]]$wt.cbps = cbps.over[[k]]$weights
#### (3)
# CBPS Weights (JUST)
cbps.just[[k]]= weightit(formula, data = df[[k]], method = "cbps", estimand = "ATT", over=FALSE)
df[[k]]$wt.cbps = cbps.over[[k]]$weights
#### (4)
# Entropy Balancing Weights
eb.out[[k]] = ebalance(Treatment = df[[k]]$treat, X = df[[k]][,confounders[[k]]])
df[[k]]$wt.eb = 1
df[[k]][df[[k]]$treat == 0,'wt.eb'] = eb.out[[k]]$w
}
