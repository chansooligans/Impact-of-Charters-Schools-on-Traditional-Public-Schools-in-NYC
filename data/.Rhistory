par(mfrow=c(2,3))
for(i in 1:6){
plot(density(Z_[treat==1,i]),col=1)
lines(density(Z_[treat==0,i]),col=2)
}
# True Propensity Score Density
plot(density(e_Z[treat==0]))
lines(density(e_Z[treat==1]),col=2)
#### Simulation 2
# Randomly generate three covariates, z1, z2, z3 with sample sizes n = 100 and n = 250, respectively, as:
z1 = rnorm(n, 0, 1)
z2 = rnorm(n, 0, 1)
z3 = rbinom(n, 5, 0.75)
z4 = rpois(n, 5)
z5 = rnorm(n, -2, 2)
z6 = rchisq(n, df=1)
Z_ = cbind(z1, z2, z3, z4, z5, z6)
# Estimate "True Propensity Scores"
x1 = exp(z1/2)
x2 = z2/(1+exp(z1))
x3 = -2*sqrt(z3)
x4 = sqrt(z3*z4)
x5 = -0.25*(z1+z5)^2
x6 = -2*z6
e_Z = inv.logit(x1 + x2 + x3 + x4 + x5 + x6)
# Generate treatment vector
treat = rbinom(n,1,prob=e_Z)
table(treat)/length(treat)
# True Propensity Score Density
plot(density(e_Z[treat==0]))
lines(density(e_Z[treat==1]),col=2)
# Treatment Effect
eff = 4
# Generate Potential Outcomes
y_0 = 0.4*z1 + 0.5*z2 + 0.3*z3 + 0.9*z4 + 0.1*z5 + z6 + rnorm(n,0,1)
y_1 = 0.4*z1 + 0.5*z2 + 0.3*z3 + 0.9*z4 + 0.1*z5 + z6 + eff + rnorm(n,0,1)
y = y_0*(1-treat) + y_1*treat
# Generate Researcher Dataset
df[[2]] = as.data.frame(cbind(y,y_1,y_0, treat, Z_[,confounders[[2]]]))
set.seed(1234)
#### Simulation 1
# Randomly generate three covariates, z1, z2, z3 with sample sizes n = 100 and n = 250, respectively, as:
z1 = rnorm(n,  3, 1)
z2 = rnorm(n,  3, 1)
z3 = rnorm(n,  3, 1)
z4 = rnorm(n, -3, 1)
Z_ = cbind(z1, z2, z3, z4)
# Estimate "True Propensity Scores"
x1 = -0.5*exp(1/z1)
x2 = exp(0.1*z2)
x3 = 0.3*z3
x4 = -0.5*z2*z3
x5 = 0.2*z4^2
e_Z = inv.logit(x1 + x2 + x3 + x4 + x5)
# Generate treatment vector
treat = rbinom(n,1,prob=e_Z)
table(treat)/length(treat)
# True Propensity Score Density
plot(density(e_Z[treat==0]))
lines(density(e_Z[treat==1]),col=2)
# Treatment Effect
eff = 4
# Generate Potential Outcomes
y_0 = 0.4*z1 + 0.5*z2 + 0.3*z3 + 0.9*z4 + rnorm(n,0,1)
y_1 = 0.4*z1 + 0.5*z2 + 0.3*z3 + 0.9*z4 + eff + rnorm(n,0,1)
y = y_0*(1-treat) + y_1*treat
# Generate Researcher Dataset
df[[1]] = as.data.frame(cbind(y,y_1,y_0, treat, Z_[,confounders[[1]]]))
# Samle size
n = 5000
# Inverse Logit Function
inv.logit = function (x) {
y = 1/(1+exp(-x))
return(y)
}
mod.inv.logit = function (x) {
y = 0.90/(1+exp(-x)) + 0.05
return(y)
}
df = list()
confounders = list()
confounders[[1]] = paste('z',seq(1,4),sep='')
confounders[[2]] = paste('z',seq(1,4),sep='')
set.seed(1234)
library(arm)
library(cobalt)
library(ggplot2)
library(rlang)
library(RColorBrewer)
library(stats)
library(Hmisc)
library(dplyr)
library(MatchIt)
library(reshape2)
library(knitr)
library(CBPS)
library(ebal)
# Set global ggplot theme
theme_set(theme_minimal())
# Define global color palettes
col.RdBl.2 <- brewer.pal(3, 'RdBu')[-2]
set.seed(1234)
#### Simulation 1
# Randomly generate three covariates, z1, z2, z3 with sample sizes n = 100 and n = 250, respectively, as:
z1 = rnorm(n,  3, 1)
z2 = rnorm(n,  3, 1)
z3 = rnorm(n,  3, 1)
z4 = rnorm(n, -3, 1)
Z_ = cbind(z1, z2, z3, z4)
# Estimate "True Propensity Scores"
x1 = -0.5*exp(1/z1)
x2 = exp(0.1*z2)
x3 = 0.3*z3
x4 = -0.5*z2*z3
x5 = 0.2*z4^2
e_Z = inv.logit(x1 + x2 + x3 + x4 + x5)
# Generate treatment vector
treat = rbinom(n,1,prob=e_Z)
table(treat)/length(treat)
# True Propensity Score Density
plot(density(e_Z[treat==0]))
lines(density(e_Z[treat==1]),col=2)
# Treatment Effect
eff = 4
# Generate Potential Outcomes
y_0 = 0.4*z1 + 0.5*z2 + 0.3*z3 + 0.9*z4 + rnorm(n,0,1)
y_1 = 0.4*z1 + 0.5*z2 + 0.3*z3 + 0.9*z4 + eff + rnorm(n,0,1)
y = y_0*(1-treat) + y_1*treat
# Generate Researcher Dataset
df[[1]] = as.data.frame(cbind(y,y_1,y_0, treat, Z_[,confounders[[1]]]))
df[[1]] = df[[1]] %>% arrange(-treat)
#### Simulation 2
# Randomly generate three covariates, z1, z2, z3 with sample sizes n = 100 and n = 250, respectively, as:
z1 = rnorm(n, 0, 1)
z2 = rnorm(n, 0, 1)
z3 = rbinom(n, 5, 0.75)
z4 = rpois(n, 5)
z5 = rnorm(n, -2, 2)
z6 = rchisq(n, df=1)
Z_ = cbind(z1, z2, z3, z4, z5, z6)
# Estimate "True Propensity Scores"
x1 = exp(z1/2)
x2 = z2/(1+exp(z1))
x3 = -2*sqrt(z3)
x4 = sqrt(z3*z4)
x5 = -0.25*(z1+z5)^2
x6 = -2*z6
e_Z = inv.logit(x1 + x2 + x3 + x4 + x5 + x6)
# Generate treatment vector
treat = rbinom(n,1,prob=e_Z)
table(treat)/length(treat)
# True Propensity Score Density
plot(density(e_Z[treat==0]))
lines(density(e_Z[treat==1]),col=2)
# Treatment Effect
eff = 4
# Generate Potential Outcomes
y_0 = 0.4*z1 + 0.5*z2 + 0.3*z3 + 0.9*z4 + 0.1*z5 + z6 + rnorm(n,0,1)
y_1 = 0.4*z1 + 0.5*z2 + 0.3*z3 + 0.9*z4 + 0.1*z5 + z6 + eff + rnorm(n,0,1)
y = y_0*(1-treat) + y_1*treat
# Generate Researcher Dataset
df[[2]] = as.data.frame(cbind(y,y_1,y_0, treat, Z_[,confounders[[2]]]))
df[[2]] = df[[2]] %>% arrange(-treat)
# SATE
SATE = mean(y_1) - mean(y_0)
SATE
# Pick Simulation
k=2
#### (1)
# Estimate Propensity Score:
ps.m1 <- glm(treat ~ ., data=df[[k]][,confounders[[k]]], family=binomial(link='logit'))
df[[k]]$psc <- ps.m1$fitted.values
# 1-1 Nearest Neighbor Matching using Propensity Score
matches <- matching(z=df[[k]]$treat, score=df[[k]]$psc, replace=TRUE)
df[[k]][df[[k]]$treat==0, 'wt'] <- matches$cnts
df[[k]][df[[k]]$treat==1, 'wt'] <- 1
#### (2)
# CBPS Weights
cbps.fit = CBPS(treat ~ ., data = df[[k]][,confounders[[k]]], ATT = 1, method = 'over')
m.out = matchit(treat ~ fitted(cbps.fit), method = 'nearest', data = df[[k]], replace = TRUE)
df[[k]]$wt.cbps = m.out$weights
#### (3)
# Entropy Balancing Weights
eb.out = ebalance(Treatment = df[[k]]$treat, X = df[[k]][,confounders[[k]]], print.level = 2)
df[[k]]$wt.eb = 1
df[[k]][df[[k]]$treat == 0,'wt.eb'] = eb.out$w
baltab.log = bal.tab(df[[k]][,c(confounders[[k]])],
treat = df[[k]]$treat,
weights = df[[k]]$wt,
method = 'weighting',
disp.v.ratio = TRUE,
un = TRUE,
m.threshold = 0.1,
v.threshold = 1.1,
estimand = 'ATT')
baltab.cb = bal.tab(cbps.fit,
disp.v.ratio = TRUE,
un = TRUE,
m.threshold = 0.1,
v.threshold = 1.1,
estimand = 'ATT')
baltab.eb = bal.tab(eb.out,
treat = df[[k]]$treat,
covs = df[[k]][,confounders[[k]]],
disp.v.ratio = TRUE,
un = TRUE,
m.threshold = 0.1,
v.threshold = 1.1,
estimand = 'ATT')
baltab.log
baltab.cb
baltab.eb
confounders[[2]] = paste('z',seq(1,6),sep='')
#### Simulation 2
# Randomly generate three covariates, z1, z2, z3 with sample sizes n = 100 and n = 250, respectively, as:
z1 = rnorm(n, 0, 1)
z2 = rnorm(n, 0, 1)
z3 = rbinom(n, 5, 0.75)
z4 = rpois(n, 5)
z5 = rnorm(n, -2, 2)
z6 = rchisq(n, df=1)
Z_ = cbind(z1, z2, z3, z4, z5, z6)
# Estimate "True Propensity Scores"
x1 = exp(z1/2)
x2 = z2/(1+exp(z1))
x3 = -2*sqrt(z3)
x4 = sqrt(z3*z4)
x5 = -0.25*(z1+z5)^2
x6 = -2*z6
e_Z = inv.logit(x1 + x2 + x3 + x4 + x5 + x6)
# Generate treatment vector
treat = rbinom(n,1,prob=e_Z)
table(treat)/length(treat)
# True Propensity Score Density
plot(density(e_Z[treat==0]))
lines(density(e_Z[treat==1]),col=2)
# Treatment Effect
eff = 4
# Generate Potential Outcomes
y_0 = 0.4*z1 + 0.5*z2 + 0.3*z3 + 0.9*z4 + 0.1*z5 + z6 + rnorm(n,0,1)
y_1 = 0.4*z1 + 0.5*z2 + 0.3*z3 + 0.9*z4 + 0.1*z5 + z6 + eff + rnorm(n,0,1)
y = y_0*(1-treat) + y_1*treat
# Generate Researcher Dataset
df[[2]] = as.data.frame(cbind(y,y_1,y_0, treat, Z_[,confounders[[2]]]))
df[[2]] = df[[2]] %>% arrange(-treat)
# SATE
SATE = mean(y_1) - mean(y_0)
SATE
# Pick Simulation
k=2
#### (1)
# Estimate Propensity Score:
ps.m1 <- glm(treat ~ ., data=df[[k]][,confounders[[k]]], family=binomial(link='logit'))
df[[k]]$psc <- ps.m1$fitted.values
# 1-1 Nearest Neighbor Matching using Propensity Score
matches <- matching(z=df[[k]]$treat, score=df[[k]]$psc, replace=TRUE)
df[[k]][df[[k]]$treat==0, 'wt'] <- matches$cnts
df[[k]][df[[k]]$treat==1, 'wt'] <- 1
#### (2)
# CBPS Weights
cbps.fit = CBPS(treat ~ ., data = df[[k]][,confounders[[k]]], ATT = 1, method = 'over')
m.out = matchit(treat ~ fitted(cbps.fit), method = 'nearest', data = df[[k]], replace = TRUE)
df[[k]]$wt.cbps = m.out$weights
#### (3)
# Entropy Balancing Weights
eb.out = ebalance(Treatment = df[[k]]$treat, X = df[[k]][,confounders[[k]]], print.level = 2)
df[[k]]$wt.eb = 1
df[[k]][df[[k]]$treat == 0,'wt.eb'] = eb.out$w
baltab.log = bal.tab(df[[k]][,c(confounders[[k]])],
treat = df[[k]]$treat,
weights = df[[k]]$wt,
method = 'weighting',
disp.v.ratio = TRUE,
un = TRUE,
m.threshold = 0.1,
v.threshold = 1.1,
estimand = 'ATT')
baltab.cb = bal.tab(cbps.fit,
disp.v.ratio = TRUE,
un = TRUE,
m.threshold = 0.1,
v.threshold = 1.1,
estimand = 'ATT')
baltab.eb = bal.tab(eb.out,
treat = df[[k]]$treat,
covs = df[[k]][,confounders[[k]]],
disp.v.ratio = TRUE,
un = TRUE,
m.threshold = 0.1,
v.threshold = 1.1,
estimand = 'ATT')
baltab.log
baltab.cb
baltab.eb
love.plot(baltab.log, threshold = 0.1)
love.plot(baltab.cb, threshold = 0.1)
love.plot(baltab.eb, threshold = 0.1)
bal.plot(eb.out,treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z1",which='both',type='histogram')
bal.plot(eb.out,treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z2",which='both',type='histogram')
bal.plot(eb.out,treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z3",which='both',type='histogram')
bal.plot(eb.out,treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z4",which='both',type='histogram')
bal.plot(eb.out,treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z5",which='both',type='histogram')
bal.plot(eb.out,treat = df[[k]]$treat,covs = df[[k]][,confounders[[k]]],
var.name = "z6",which='both',type='histogram')
examine_overlap(df[[k]], var='z5', var_name = 'z4', xlab = 'z4', ylim=c(-1000,2000))
# Paired, inverted histograms: propensity scores:
examine_overlap = function(dt, nbins=40, var='psc', var_name='Propensity Score', xlab='Propensity Score', ylim=c(-30,50), options=NULL){
dt_plot = dt[, c('treat', var)]
colnames(dt_plot) = c('treat', 'X')
ggplot(dt_plot) +
geom_histogram(data=dt_plot[dt_plot$treat==0,], bins=nbins, fill='grey', alpha=0.1, aes(X, y=..count.., color=col.RdBl.2[2])) +
geom_histogram(data=dt_plot[dt_plot$treat==1,], bins=nbins, fill='grey', alpha=0.1, aes(X, y=-..count.., color=col.RdBl.2[1])) +
coord_cartesian(ylim=ylim) +
scale_color_manual(values=rev(col.RdBl.2), labels=c('Control', 'Treated'), name='') +
labs(x=xlab, y='Frequency', title=paste('Overlap of ', var_name, ' between Groups', sep='')) +
options
}
checkBalance = function(df, confounders, wt_var) {
# Subset data to confounders only
df2 <- df[, confounders]
binary = apply(df2, 2, function(x) all(x %in% 0:1)) # Binary Variable Indicator
trt = df$treat == 1                   # Treatment Indicator
ctr = df$treat == 0                   # Control Indicator
n_trt = sum(trt)                      # Treatment Sample Size
n_ctrl = sum(ctr)                     # Control Sample Size
wts.trt = df[trt, wt_var]              # Set treatment weights
wts.ctr = df[ctr, wt_var]  # Set control weights
# Means
trt.means = apply(df2[trt,], 2, mean)
trt.means_w = apply(df2[trt,], 2, wtd.mean, wts.trt)
ctr.means = apply(df2[ctr,], 2, mean)
ctr.means_w = apply(df2[ctr,], 2, wtd.mean, wts.ctr)
# Variances
trt.var = apply(df2[trt,], 2, var)
trt.var_w = apply(df2[trt,], 2, function(x) sum(wts.trt * (x - wtd.mean(x, wts.trt))^2) / (sum(wts.trt) - 1))
ctr.var = apply(df2[ctr,], 2, var)
ctr.var_w = apply(df2[ctr,], 2, function(x) sum(wts.ctr * (x - wtd.mean(x, wts.ctr))^2) / (sum(wts.ctr) - 1))
# Standardized Mean Differences
mean.diff = (trt.means-ctr.means) / sqrt(trt.var)
mean.diff.bin = (trt.means-ctr.means)
diff = mean.diff * (1-binary) + mean.diff.bin * binary
mean.diff_w = (trt.means_w-ctr.means_w) / sqrt(trt.var_w)
mean.diff.bin_w = (trt.means_w-ctr.means_w)
diff.m = mean.diff_w*(1-binary) + mean.diff.bin_w*binary
# Ratios of standard deviations
ratio = sqrt(ctr.var)/sqrt(trt.var)
ratio[binary] = NA
ratio.m = sqrt(ctr.var_w)/sqrt(trt.var_w)
ratio.m[binary] = NA
# Return results
result = cbind(trt.means, ctr.means, trt.means, ctr.means_w, diff, diff.m, ratio, ratio.m)
colnames(result) = c('mn1', 'mn0', 'mn1.m', 'mn0.m', 'diff', 'diff.m', 'ratio', 'ratio.m')
return(result)
}
# Plot standardized mean Differences
plotMeanDiff = function(balance_object, name, bounds = c(-1,1)){
df = data.frame(balance_object[,c(5,6,9,10)])
colnames(df) = c('unmatched','value','model','cov')
unmatched = data.frame(unique(df$unmatched),'unmatched',unique(df$cov))
colnames(unmatched) = c('value','model','cov')
df = rbind(df[,2:4],unmatched)
df = melt(df)
ggplot(df, aes(x = value, y = cov)) +
geom_point(aes(shape = model, color=model),size=2) +
geom_vline(xintercept=0, color='blue') +
geom_vline(xintercept=-.05, color='orange', linetype='dotted') +
geom_vline(xintercept=.05, color='orange', linetype='dotted') +
geom_vline(xintercept=-.1, color='red', linetype='dotted') +
geom_vline(xintercept=.1, color='red', linetype='dotted') +
coord_cartesian(xlim=bounds) +
labs(x = 'Standardized Difference in Means', y = 'predictor', title = name,
caption='Orange dotted line indicates +/-0.05 bounds and red lines indicate +/-0.1 bounds.\n Values outside of range x=[-0.5,0.5] not displayed.')
}
# Plot Ratios of Standard Deviations
plotSDRatios = function(balance_object,name){
df = data.frame(balance_object[,c(7,8,9,10)])
df = df[!is.na(df)[,1],]
colnames(df) = c('unmatched','value','model','cov')
unmatched = data.frame(unique(df$unmatched),'unmatched',unique(df$cov))
colnames(unmatched) = c('value','model','cov')
df = rbind(df[,2:4],unmatched)
df = melt(df)
ggplot(df, aes(x = value, y = cov, shape = model, color = model)) +
geom_point(size=2) +
geom_vline(xintercept=1, color='blue') +
labs(x = 'Ratios of Standard Deviations', y = 'predictor', title=name)
}
examine_overlap(df[[k]], var='z1', var_name = 'z1', xlab = 'z1', ylim=c(-200,300))
examine_overlap(df[[k]], var='z2', var_name = 'z2', xlab = 'z2', ylim=c(-200,300))
examine_overlap(df[[k]], var='z3', var_name = 'z3', xlab = 'z3', ylim=c(-1000,2000))
examine_overlap(df[[k]], var='z4', var_name = 'z4', xlab = 'z4', ylim=c(-1000,2000))
examine_overlap(df[[k]], var='z5', var_name = 'z4', xlab = 'z4', ylim=c(-1000,2000))
examine_overlap(df[[k]], var='z6', var_name = 'z4', xlab = 'z4', ylim=c(-1000,2000))
examine_overlap(df[[k]], var='psc', ylim=c(-15,25))
examine_overlap(df[[k]], var='psc', ylim=c(-200,300))
examine_overlap(df[[k]], var='z5', var_name = 'z4', xlab = 'z4', ylim=c(-200,300))
examine_overlap(df[[k]], var='z6', var_name = 'z4', xlab = 'z4', ylim=c(-200,300))
# Get Balance Tables
logit.bal <- checkBalance(df[[k]], confounders[[k]], 'wt')
cbps.bal <- checkBalance(df[[k]], confounders[[k]], 'wt.cbps')
eb.bal <- checkBalance(df[[k]], confounders[[k]], 'wt.eb')
# Aggregate Balance Tables
temp = rbind(logit.bal, cbps.bal, eb.bal)
temp = temp[order(rownames(temp)),]
# row.names(temp) = paste(row.names(temp),rep(c('logit','cbps','eb'),times=3),sep='_')
temp = as.data.frame(temp)
temp$model = rep(c('logit','cbps','eb'),times=length(confounders[[k]]))
temp$var = rep(confounders[[k]],each=3)
# Plot Mean Differences
plotMeanDiff(temp, 'Comparing Mean Differences of All Models', bounds = c(-1,1))
# Plot SD Ratios
plotSDRatios(temp, 'Comparing SD Ratios of All Propensity Score Models\n and Matching Methods Used')
k=2
n_sims = 100
baseline_coefs = rep(1,n_sims)
cbps_coefs = rep(1,n_sims)
eb_coefs = rep(1,n_sims)
naive_est = rep(1,n_sims)
for(i in 1:n_sims){
# Generate treatment vector
treat = as.integer(runif(n) <= e_Z)
y = y_0*(1-treat) + y_1*treat
df[[k]] = as.data.frame(cbind(y, y_1, y_0, treat, Z_[,confounders[[k]]]))
## NAIVE-estimate
naive_est[i] = mean(df[[k]]$y[df[[k]]$treat==1]) - mean(df[[k]]$y[df[[k]]$treat==0])
## Baseline: Propensity Score using Logistic Regression:
# Estimate Propensity Score:
ps.m1 <- glm(treat ~ ., data=df[[k]][,confounders[[k]]], family=binomial(link='logit'))
df[[k]]$psc <- ps.m1$fitted.values
# 1-1 Nearest Neighbor Matching using Propensity Score
matches <- matching(z=df[[k]]$treat, score=df[[k]]$psc, replace=TRUE)
df[[k]][df[[k]]$treat==0, 'wt'] <- matches$cnts
df[[k]][df[[k]]$treat==1, 'wt'] <- 1
## Covariate Balancing Propensity Score (CBPS)
# CBPS Weights
cbps.fit = CBPS(treat ~ ., data = df[[k]][,confounders[[k]]], ATT = TRUE)
m.out = matchit(treat ~ fitted(cbps.fit), method = 'nearest', data = df[[k]], replace = TRUE)
df[[k]]$wt.cbps = m.out$weights
## Entropy Balancing
# Entropy Balancing Weights
eb.out = ebalance(Treatment = df[[k]]$treat, X = df[[k]][,confounders[[k]]])
df[[k]]$wt.eb = 1
df[[k]][df[[k]]$treat == 0,'wt.eb'] = eb.out$w
# Linear Regression
# mod.baseline.sim = lm(y ~ ., data = df[[k]][,c('y','treat',confounders[[k]])], weights = df[[k]]$wt)
# mod.cbps = lm(y ~ ., data = df[[k]][,c('y','treat',confounders[[k]])], weights = df[[k]]$wt.cbps)
# mod.eb = lm(y ~ ., data = df[[k]][,c('y','treat',confounders[[k]])], weights = df[[k]]$wt.eb)
# Save Coefficients
# baseline_coefs[i] = summary(mod.baseline.sim)$coefficients['treat',1]
# cbps_coefs[i] = summary(mod.cbps)$coefficients['treat',1]
# eb_coefs[i] = summary(mod.eb)$coefficients['treat',1]
baseline_coefs[i] = weighted.mean(df[[k]]$y[df[[k]]$treat==1],df[[k]]$wt[df[[k]]$treat==1]) -
weighted.mean(df[[k]]$y[df[[k]]$treat==0],df[[k]]$wt[df[[k]]$treat==0])
cbps_coefs[i] = weighted.mean(df[[k]]$y[df[[k]]$treat==1],df[[k]]$wt.cbps[df[[k]]$treat==1]) -
weighted.mean(df[[k]]$y[df[[k]]$treat==0],df[[k]]$wt.cbps[df[[k]]$treat==0])
eb_coefs[i] = weighted.mean(df[[k]]$y[df[[k]]$treat==1],df[[k]]$wt.eb[df[[k]]$treat==1]) -
weighted.mean(df[[k]]$y[df[[k]]$treat==0],df[[k]]$wt.eb[df[[k]]$treat==0])
}
plot(density(baseline_coefs), ylim=c(0,4), xlim=c(0,6))
abline(v=mean(baseline_coefs), col='black', lty=2)
lines(density(naive_est), col = 'purple')
abline(v=mean(naive_est), col='purple', lty=2)
lines(density(cbps_coefs), col = 'blue')
abline(v=mean(cbps_coefs), col='blue', lty=2)
lines(density(eb_coefs), col = 'green')
abline(v=mean(eb_coefs), col='green', lty=2)
abline(v=mean(SATE), col='red', lty=2)
plot(density(baseline_coefs), ylim=c(0,4), xlim=c(3,5))
abline(v=mean(baseline_coefs), col='black', lty=2)
lines(density(naive_est), col = 'purple')
abline(v=mean(naive_est), col='purple', lty=2)
lines(density(cbps_coefs), col = 'blue')
abline(v=mean(cbps_coefs), col='blue', lty=2)
lines(density(eb_coefs), col = 'green')
abline(v=mean(eb_coefs), col='green', lty=2)
abline(v=mean(SATE), col='red', lty=2)
plot(density(baseline_coefs), ylim=c(0,7), xlim=c(3,5))
abline(v=mean(baseline_coefs), col='black', lty=2)
lines(density(naive_est), col = 'purple')
abline(v=mean(naive_est), col='purple', lty=2)
lines(density(cbps_coefs), col = 'blue')
abline(v=mean(cbps_coefs), col='blue', lty=2)
lines(density(eb_coefs), col = 'green')
abline(v=mean(eb_coefs), col='green', lty=2)
abline(v=mean(SATE), col='red', lty=2)
plot(density(baseline_coefs), ylim=c(0,10), xlim=c(3,5))
abline(v=mean(baseline_coefs), col='black', lty=2)
plot(density(baseline_coefs), ylim=c(0,10), xlim=c(3,5))
abline(v=mean(baseline_coefs), col='black', lty=2)
lines(density(naive_est), col = 'purple')
abline(v=mean(naive_est), col='purple', lty=2)
lines(density(cbps_coefs), col = 'blue')
abline(v=mean(cbps_coefs), col='blue', lty=2)
lines(density(eb_coefs), col = 'green')
abline(v=mean(eb_coefs), col='green', lty=2)
abline(v=mean(SATE), col='red', lty=2)
plot(density(baseline_coefs), ylim=c(0,10), xlim=c(3,5),main='Dists of Treatment Eff Est')
abline(v=mean(baseline_coefs), col='black', lty=2)
lines(density(naive_est), col = 'purple')
abline(v=mean(naive_est), col='purple', lty=2)
lines(density(cbps_coefs), col = 'blue')
abline(v=mean(cbps_coefs), col='blue', lty=2)
lines(density(eb_coefs), col = 'green')
abline(v=mean(eb_coefs), col='green', lty=2)
abline(v=mean(SATE), col='red', lty=2)
getwd(0)
getwd()
